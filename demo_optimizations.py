#!/usr/bin/env python3
"""Demo script showcasing XHS Spider optimizations"""

import sys
import os
from datetime import datetime, timedelta
from pathlib import Path

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

print("ğŸš€ XHS Spider Optimization Demo")
print("=" * 50)

try:
    from optimizations.config_manager import (
        ConfigManager, SearchPresets, CrawlerConfig
    )
    from optimizations.smart_crawler import (
        SmartCrawler, ContentItem, DuplicateDetector, ContentQualityAnalyzer
    )
    print("âœ… Successfully imported optimization modules")
except ImportError as e:
    print(f"âŒ Import error: {e}")
    print("Please ensure all optimization dependencies are installed:")
    print("pip install pyyaml scikit-learn pillow imagehash rich click aiohttp aiofiles")
    sys.exit(1)

def demo_configuration_management():
    """Demonstrate configuration management features"""
    print("\nğŸ”§ Configuration Management Demo")
    print("-" * 30)
    
    # Create configuration manager
    config_manager = ConfigManager("demo_config.yaml")
    
    # Test default configuration
    config = config_manager.create_default_config()
    print(f"âœ“ Default config created with {len(config.search.keywords)} keywords")
    
    # Test search presets
    fashion_config = SearchPresets.fashion_trends()
    print(f"âœ“ Fashion preset: {len(fashion_config.keywords)} keywords, min_likes: {fashion_config.min_likes}")
    
    food_config = SearchPresets.food_content()
    print(f"âœ“ Food preset: {len(food_config.keywords)} keywords")
    
    # Save and load configuration
    config.search = fashion_config
    config_manager.save_config(config)
    print("âœ“ Configuration saved to demo_config.yaml")
    
    loaded_config = config_manager.load_config()
    print(f"âœ“ Configuration loaded: {len(loaded_config.search.keywords)} keywords")
    
    return config

def demo_duplicate_detection():
    """Demonstrate duplicate detection capabilities"""
    print("\nğŸ” Duplicate Detection Demo")
    print("-" * 30)
    
    detector = DuplicateDetector(similarity_threshold=0.85)
    
    # Test text duplicate detection
    original_text = "ä»Šå¤©åˆ†äº«ä¸€ä¸ªè¶…æ£’çš„ç©¿æ­ï¼Œè¿™å¥—lookéå¸¸é€‚åˆæ˜¥å¤©ï¼"
    duplicate_text = "ä»Šå¤©åˆ†äº«ä¸€ä¸ªè¶…æ£’çš„ç©¿æ­ï¼Œè¿™å¥—lookéå¸¸é€‚åˆæ˜¥å¤©ï¼"
    different_text = "è¿™æ˜¯ä¸€ä¸ªå®Œå…¨ä¸åŒçš„å†…å®¹ï¼Œè®²çš„æ˜¯ç¾é£Ÿåˆ¶ä½œã€‚"
    
    print("Testing text duplicate detection:")
    print(f"  Original text: {original_text[:30]}...")
    
    is_original_duplicate = detector.is_duplicate_text(original_text)
    print(f"  âœ“ Original text duplicate: {is_original_duplicate}")
    
    is_duplicate = detector.is_duplicate_text(duplicate_text)
    print(f"  âœ“ Exact duplicate detected: {is_duplicate}")
    
    is_different_duplicate = detector.is_duplicate_text(different_text)
    print(f"  âœ“ Different text duplicate: {is_different_duplicate}")
    
    return detector

def demo_quality_analysis():
    """Demonstrate content quality analysis"""
    print("\nğŸ“Š Content Quality Analysis Demo")
    print("-" * 30)
    
    analyzer = ContentQualityAnalyzer()
    
    # Test different quality levels
    test_contents = [
        {
            "content": "ä»Šå¤©åˆ†äº«ä¸€ä¸ªéå¸¸è¯¦ç»†çš„ç©¿æ­æ•™ç¨‹ï¼Œè¿™å¥—lookèåˆäº†éŸ©ç³»å’Œæ—¥ç³»çš„é£æ ¼ç‰¹ç‚¹ï¼Œæ—¢ä¿æŒäº†éŸ©ç³»çš„ç®€çº¦å¤§æ–¹ï¼Œåˆèå…¥äº†æ—¥ç³»çš„ç²¾è‡´ç»†èŠ‚ã€‚æ•´å¥—æ­é…ä»¥ç±³è‰²ä¸ºä¸»è°ƒï¼Œè¥é€ å‡ºæ¸©æŸ”çŸ¥æ€§çš„æ°›å›´ã€‚",
            "likes": 500,
            "comments": 45,
            "label": "High Quality"
        },
        {
            "content": "ä»Šå¤©çš„ç©¿æ­åˆ†äº«ï¼Œå¸Œæœ›å¤§å®¶å–œæ¬¢è¿™å¥—look",
            "likes": 80,
            "comments": 8,
            "label": "Medium Quality"
        },
        {
            "content": "å…³æ³¨æˆ‘ï¼Œç§ä¿¡å›å¤è·å–æ›´å¤š",
            "likes": 5,
            "comments": 1,
            "label": "Low Quality (Spam)"
        }
    ]
    
    for test in test_contents:
        text_score = analyzer.score_text_quality(test["content"])
        engagement_score = analyzer.score_engagement_quality(test["likes"], test["comments"])
        
        print(f"  {test['label']}:")
        print(f"    Text score: {text_score:.2f}")
        print(f"    Engagement score: {engagement_score:.2f}")
        print(f"    Content preview: {test['content'][:40]}...")
        print()
    
    return analyzer

def demo_smart_crawler():
    """Demonstrate smart crawler functionality"""
    print("\nğŸ¤– Smart Crawler Demo")
    print("-" * 30)
    
    # Create configuration
    config_manager = ConfigManager()
    config = config_manager.create_default_config()
    config.search = SearchPresets.fashion_trends()
    config.filters.enable_duplicate_detection = True
    config.filters.enable_quality_filter = True
    config.filters.quality_threshold = 0.3  # Lower threshold for demo
    
    # Create sample content items
    sample_items = [
        ContentItem(
            id="fashion_1",
            url="https://example.com/fashion_1",
            title="æ—¶å°šç©¿æ­åˆ†äº«",
            content="ä»Šå¤©åˆ†äº«ä¸€ä¸ªè¶…çº§æ£’çš„æ˜¥å­£ç©¿æ­ï¼Œè¿™å¥—OOTDèåˆäº†å¤šç§é£æ ¼å…ƒç´ ï¼Œæ—¢æ—¶å°šåˆå®ç”¨ã€‚æ•´ä½“ä»¥æ¸…æ–°çš„è‰²è°ƒä¸ºä¸»ï¼Œéå¸¸é€‚åˆæ—¥å¸¸é€šå‹¤ã€‚",
            author="fashion_blogger",
            author_id="fb123",
            publish_time=datetime.now(),
            likes=300,
            comments=35,
            hashtags=["#ç©¿æ­", "#æ—¶å°š", "#OOTD", "#æ˜¥å­£æ­é…"]
        ),
        ContentItem(
            id="fashion_2",
            url="https://example.com/fashion_2",
            title="é‡å¤å†…å®¹æµ‹è¯•",
            content="ä»Šå¤©åˆ†äº«ä¸€ä¸ªè¶…çº§æ£’çš„æ˜¥å­£ç©¿æ­ï¼Œè¿™å¥—OOTDèåˆäº†å¤šç§é£æ ¼å…ƒç´ ï¼Œæ—¢æ—¶å°šåˆå®ç”¨ã€‚æ•´ä½“ä»¥æ¸…æ–°çš„è‰²è°ƒä¸ºä¸»ï¼Œéå¸¸é€‚åˆæ—¥å¸¸é€šå‹¤ã€‚",  # Exact duplicate
            author="copy_user",
            author_id="cu456",
            publish_time=datetime.now(),
            likes=50,
            comments=5,
            hashtags=["#ç©¿æ­"]
        ),
        ContentItem(
            id="food_1",
            url="https://example.com/food_1",
            title="ç¾é£Ÿåˆ¶ä½œ",
            content="ä»Šå¤©æ•™å¤§å®¶åšä¸€é“ç®€å•åˆç¾å‘³çš„å®¶å¸¸èœï¼Œææ–™ç®€å•ï¼Œåˆ¶ä½œè¿‡ç¨‹è¯¦ç»†ï¼Œæ–°æ‰‹ä¹Ÿèƒ½è½»æ¾æŒæ¡ã€‚",
            author="food_lover",
            author_id="fl789",
            publish_time=datetime.now(),
            likes=150,
            comments=20,
            hashtags=["#ç¾é£Ÿ", "#cooking", "#å®¶å¸¸èœ"]
        ),
        ContentItem(
            id="spam_1",
            url="https://example.com/spam_1",
            title="ä½è´¨é‡å†…å®¹",
            content="ç‚¹å‡»å…³æ³¨ï¼Œç§ä¿¡å›å¤1è·å–æ›´å¤šèµ„æº",
            author="spam_user",
            author_id="su000",
            publish_time=datetime.now(),
            likes=3,
            comments=0,
            hashtags=[]
        ),
        ContentItem(
            id="old_content",
            url="https://example.com/old",
            title="è¿‡æœŸå†…å®¹",
            content="è¿™æ˜¯ä¸€ä¸ªå¾ˆä¹…ä¹‹å‰çš„å†…å®¹ï¼Œå¯èƒ½å·²ç»ä¸å¤ªç›¸å…³äº†",
            author="old_user",
            author_id="ou111",
            publish_time=datetime.now() - timedelta(days=400),  # Very old
            likes=100,
            comments=10,
            hashtags=["#è¿‡æœŸ"]
        )
    ]
    
    # Create smart crawler and process items
    crawler = SmartCrawler(config, max_workers=1)
    
    print(f"ğŸ“¥ Processing {len(sample_items)} sample items...")
    print()
    
    processed_items = crawler.process_batch(sample_items)
    
    print(f"ğŸ“Š Processing Results:")
    print(f"  Input items: {len(sample_items)}")
    print(f"  Output items: {len(processed_items)}")
    print(f"  Duplicates filtered: {crawler.stats['duplicates_filtered']}")
    print(f"  Quality filtered: {crawler.stats['quality_filtered']}")
    print()
    
    # Show categorization results
    print("ğŸ“‚ Categorization Results:")
    for item in processed_items:
        print(f"  {item.id}: {', '.join(item.categories)} (Quality: {item.quality_score:.2f})")
    print()
    
    # Generate analytics
    crawler.downloaded_items = processed_items
    analytics = crawler.generate_analytics_report()
    
    print("ğŸ“ˆ Analytics Summary:")
    summary = analytics['summary']
    print(f"  Total items: {summary['total_items']}")
    print(f"  Average quality: {summary['average_quality_score']:.2f}")
    print(f"  Average likes: {summary['average_likes']:.1f}")
    print(f"  Total engagement: {summary['total_engagement']}")
    
    if analytics['category_distribution']:
        print(f"  Categories: {', '.join(analytics['category_distribution'].keys())}")
    
    return crawler, analytics

def demo_export_features():
    """Demonstrate export and analytics features"""
    print("\nğŸ“¤ Export Features Demo")
    print("-" * 30)
    
    # Create output directory
    output_dir = Path("demo_output")
    output_dir.mkdir(exist_ok=True)
    
    # Create sample analytics data
    sample_analytics = {
        'summary': {
            'total_items': 25,
            'average_quality_score': 0.72,
            'average_likes': 156.3,
            'average_comments': 18.4,
            'total_engagement': 4357
        },
        'category_distribution': {
            'fashion': 15,
            'food': 7,
            'beauty': 3
        },
        'top_authors': [
            ('fashion_expert', 8),
            ('food_blogger', 5),
            ('style_guru', 4)
        ],
        'processing_stats': {
            'total_found': 35,
            'duplicates_filtered': 6,
            'quality_filtered': 4,
            'successfully_downloaded': 25
        },
        'quality_breakdown': {
            'high_quality': 8,
            'medium_quality': 12,
            'low_quality': 5
        }
    }
    
    # Export analytics to JSON
    import json
    analytics_file = output_dir / "demo_analytics.json"
    with open(analytics_file, 'w', encoding='utf-8') as f:
        json.dump(sample_analytics, f, indent=2, ensure_ascii=False)
    
    print(f"âœ“ Analytics exported to {analytics_file}")
    
    # Create a simple HTML gallery template
    categories_html = ''.join([f'<span class="category">{cat}: {count}</span>' 
                              for cat, count in sample_analytics['category_distribution'].items()])
    
    html_template = f"""<!DOCTYPE html>
<html>
<head>
    <title>XHS Spider Analytics Dashboard</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        .metric {{ background: #f5f5f5; padding: 15px; margin: 10px; border-radius: 5px; }}
        .category {{ display: inline-block; margin: 5px; padding: 5px 10px; background: #e3f2fd; border-radius: 3px; }}
    </style>
</head>
<body>
    <h1>ğŸ“Š XHS Spider Analytics Dashboard</h1>
    
    <div class="metric">
        <h3>Summary</h3>
        <p>Total Items: {sample_analytics['summary']['total_items']}</p>
        <p>Average Quality: {sample_analytics['summary']['average_quality_score']:.2f}</p>
        <p>Total Engagement: {sample_analytics['summary']['total_engagement']}</p>
    </div>
    
    <div class="metric">
        <h3>Categories</h3>
        {categories_html}
    </div>
    
    <div class="metric">
        <h3>Processing Stats</h3>
        <p>Found: {sample_analytics['processing_stats']['total_found']}</p>
        <p>Duplicates Filtered: {sample_analytics['processing_stats']['duplicates_filtered']}</p>
        <p>Quality Filtered: {sample_analytics['processing_stats']['quality_filtered']}</p>
    </div>
</body>
</html>"""
    
    html_file = output_dir / "demo_dashboard.html"
    with open(html_file, 'w', encoding='utf-8') as f:
        f.write(html_template)
    
    print(f"âœ“ HTML dashboard created at {html_file}")
    
    return analytics_file, html_file

def main():
    """Run the complete optimization demo"""
    print("Starting comprehensive optimization demo...\n")
    
    try:
        # 1. Configuration Management
        config = demo_configuration_management()
        
        # 2. Duplicate Detection
        detector = demo_duplicate_detection()
        
        # 3. Quality Analysis
        analyzer = demo_quality_analysis()
        
        # 4. Smart Crawler
        crawler, analytics = demo_smart_crawler()
        
        # 5. Export Features
        analytics_file, html_file = demo_export_features()
        
        print("\nğŸ‰ Demo Completed Successfully!")
        print("=" * 50)
        print("âœ… All optimization features demonstrated:")
        print("   â€¢ Configuration management with presets")
        print("   â€¢ Intelligent duplicate detection")
        print("   â€¢ Content quality analysis")
        print("   â€¢ Smart crawler with filtering")
        print("   â€¢ Analytics generation")
        print("   â€¢ Export capabilities")
        print()
        print("ğŸ“ Output files created:")
        print(f"   â€¢ Configuration: demo_config.yaml")
        print(f"   â€¢ Analytics: {analytics_file}")
        print(f"   â€¢ Dashboard: {html_file}")
        print()
        print("ğŸš€ The optimization system is ready for production use!")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Demo failed with error: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)